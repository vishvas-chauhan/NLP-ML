{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_text_analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishvas-chauhan/NLP-ML/blob/main/TensorFlow_text_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUy0kv4O3Eze"
      },
      "source": [
        "#import 3 libraries \n",
        "#tensorflow is opensource, developed by google brain team, good and work on potential words only and avoid words like 'i', 'am', 'is' etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzGJVoikD_eK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds #sample datasets from google tensorflow. \n",
        "import tensorflow_hub as hub # module use for pretrained datasets. Till now I used only for NLP purpose only. It Can save a lot of time. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2xc6hjE3LOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d64085df-16e9-4072-920b-fafb76ad7bcc"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-R2eBQTaTL_"
      },
      "source": [
        "#import tensorflow_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-gN5xJlGI-C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16166527-4937-4a4b-f4fe-83feef8a9aa1"
      },
      "source": [
        "tfds.list_builders() #inbuild dataset list provided by google team, I will use 'imdb review'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abstract_reasoning',\n",
              " 'aeslc',\n",
              " 'aflw2k3d',\n",
              " 'amazon_us_reviews',\n",
              " 'arc',\n",
              " 'bair_robot_pushing_small',\n",
              " 'beans',\n",
              " 'big_patent',\n",
              " 'bigearthnet',\n",
              " 'billsum',\n",
              " 'binarized_mnist',\n",
              " 'binary_alpha_digits',\n",
              " 'c4',\n",
              " 'caltech101',\n",
              " 'caltech_birds2010',\n",
              " 'caltech_birds2011',\n",
              " 'cars196',\n",
              " 'cassava',\n",
              " 'cats_vs_dogs',\n",
              " 'celeb_a',\n",
              " 'celeb_a_hq',\n",
              " 'cfq',\n",
              " 'chexpert',\n",
              " 'cifar10',\n",
              " 'cifar100',\n",
              " 'cifar10_1',\n",
              " 'cifar10_corrupted',\n",
              " 'citrus_leaves',\n",
              " 'cityscapes',\n",
              " 'civil_comments',\n",
              " 'clevr',\n",
              " 'cmaterdb',\n",
              " 'cnn_dailymail',\n",
              " 'coco',\n",
              " 'coil100',\n",
              " 'colorectal_histology',\n",
              " 'colorectal_histology_large',\n",
              " 'cos_e',\n",
              " 'curated_breast_imaging_ddsm',\n",
              " 'cycle_gan',\n",
              " 'deep_weeds',\n",
              " 'definite_pronoun_resolution',\n",
              " 'diabetic_retinopathy_detection',\n",
              " 'div2k',\n",
              " 'dmlab',\n",
              " 'downsampled_imagenet',\n",
              " 'dsprites',\n",
              " 'dtd',\n",
              " 'duke_ultrasound',\n",
              " 'dummy_dataset_shared_generator',\n",
              " 'dummy_mnist',\n",
              " 'emnist',\n",
              " 'eraser_multi_rc',\n",
              " 'esnli',\n",
              " 'eurosat',\n",
              " 'fashion_mnist',\n",
              " 'flic',\n",
              " 'flores',\n",
              " 'food101',\n",
              " 'gap',\n",
              " 'gigaword',\n",
              " 'glue',\n",
              " 'groove',\n",
              " 'higgs',\n",
              " 'horses_or_humans',\n",
              " 'i_naturalist2017',\n",
              " 'image_label_folder',\n",
              " 'imagenet2012',\n",
              " 'imagenet2012_corrupted',\n",
              " 'imagenet_resized',\n",
              " 'imagenette',\n",
              " 'imagewang',\n",
              " 'imdb_reviews',\n",
              " 'iris',\n",
              " 'kitti',\n",
              " 'kmnist',\n",
              " 'lfw',\n",
              " 'librispeech',\n",
              " 'librispeech_lm',\n",
              " 'libritts',\n",
              " 'lm1b',\n",
              " 'lost_and_found',\n",
              " 'lsun',\n",
              " 'malaria',\n",
              " 'math_dataset',\n",
              " 'mnist',\n",
              " 'mnist_corrupted',\n",
              " 'movie_rationales',\n",
              " 'moving_mnist',\n",
              " 'multi_news',\n",
              " 'multi_nli',\n",
              " 'multi_nli_mismatch',\n",
              " 'natural_questions',\n",
              " 'newsroom',\n",
              " 'nsynth',\n",
              " 'omniglot',\n",
              " 'open_images_v4',\n",
              " 'opinosis',\n",
              " 'oxford_flowers102',\n",
              " 'oxford_iiit_pet',\n",
              " 'para_crawl',\n",
              " 'patch_camelyon',\n",
              " 'pet_finder',\n",
              " 'places365_small',\n",
              " 'plant_leaves',\n",
              " 'plant_village',\n",
              " 'plantae_k',\n",
              " 'qa4mre',\n",
              " 'quickdraw_bitmap',\n",
              " 'reddit_tifu',\n",
              " 'resisc45',\n",
              " 'rock_paper_scissors',\n",
              " 'rock_you',\n",
              " 'scan',\n",
              " 'scene_parse150',\n",
              " 'scicite',\n",
              " 'scientific_papers',\n",
              " 'shapes3d',\n",
              " 'smallnorb',\n",
              " 'snli',\n",
              " 'so2sat',\n",
              " 'speech_commands',\n",
              " 'squad',\n",
              " 'stanford_dogs',\n",
              " 'stanford_online_products',\n",
              " 'starcraft_video',\n",
              " 'sun397',\n",
              " 'super_glue',\n",
              " 'svhn_cropped',\n",
              " 'ted_hrlr_translate',\n",
              " 'ted_multi_translate',\n",
              " 'tf_flowers',\n",
              " 'the300w_lp',\n",
              " 'tiny_shakespeare',\n",
              " 'titanic',\n",
              " 'trivia_qa',\n",
              " 'uc_merced',\n",
              " 'ucf101',\n",
              " 'vgg_face2',\n",
              " 'visual_domain_decathlon',\n",
              " 'voc',\n",
              " 'wider_face',\n",
              " 'wikihow',\n",
              " 'wikipedia',\n",
              " 'wmt14_translate',\n",
              " 'wmt15_translate',\n",
              " 'wmt16_translate',\n",
              " 'wmt17_translate',\n",
              " 'wmt18_translate',\n",
              " 'wmt19_translate',\n",
              " 'wmt_t2t_translate',\n",
              " 'wmt_translate',\n",
              " 'xnli',\n",
              " 'xsum',\n",
              " 'yelp_polarity_reviews']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfUw_uhl3JEV"
      },
      "source": [
        "splits= (\"train[:80]\",'train[:10]','train[:10]')\n",
        "(train_data, validation_data, test_data),metadeta= tfds.load(name='imdb_reviews',split=splits,with_info=True,as_supervised=True) \n",
        "#we will load and split the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN22Om-T6Avk"
      },
      "source": [
        "train_example_batch, train_label_batch = next(iter(train_data.batch(2))) \n",
        "#split numpy_batched into smaple and classified way (0,1 only), lets check with first 2 comments \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMnzYe8O6ziE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c6717200-45d0-4e1f-f2a6-e7f47cd9f328"
      },
      "source": [
        "train_example_batch\n",
        "#both are negative or toxic comments..'This was an absolutely terrible movie'.. Agree?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
              "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
              "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n33vMRYe7Bpc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "782a9fa0-1e76-44d1-9fe2-674fa487aa6a"
      },
      "source": [
        "train_label_batch\n",
        "#negative comment means 0 and postive you know ?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTdyu8BnH70a"
      },
      "source": [
        "#It was defined in given datasets. Now lets make and train a model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy-72MqJISOS"
      },
      "source": [
        "#pretrained_model= you go for https://tfhub.dev and you will find NLP pretrained hub (transfering models from TensorFlow hub )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSe7rDfx7F7T"
      },
      "source": [
        "#copy paste model link according to your requirement, below one is trained on 130GB text data so It was not a bad choice. Agree?\n",
        "\n",
        "pretrained_model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "\n",
        "#making a layer and then we will add this layer in our model for predictions. Its not that tricky right?\n",
        "#it converts pretrained model to vector or say from WORDS TO NUMBERS\n",
        "\n",
        "hub_layer = hub.KerasLayer(pretrained_model, input_shape=[], dtype=tf.string, trainable=True, name='bert_embedding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB50hEez7NUQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "dfa48bc1-e881-4011-85d6-cc80757f1687"
      },
      "source": [
        "#apply word2vector hub_layer ON over first comments. if its working. \n",
        "hub_layer(train_example_batch[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
              "array([[ 1.765786  , -3.882232  ,  3.9134233 , -1.5557289 , -3.3362343 ,\n",
              "        -1.7357955 , -1.9954445 ,  1.2989551 ,  5.081598  , -1.1041286 ,\n",
              "        -2.0503852 , -0.72675157, -0.65675956,  0.24436149, -3.7208383 ,\n",
              "         2.0954835 ,  2.2969332 , -2.0689783 , -2.9489717 , -1.1315987 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6CCX9aFJ9pQ"
      },
      "source": [
        "#we dont need to understand these numbers. As i said it randomly selected 20 potential words only. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6mljJUm8f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f5eaa1b9-90c8-4703-8035-c20ee5a4f1d2"
      },
      "source": [
        "train_example_batch[:1] #there are lots of words though. It saved our 50% of the time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
              "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxL-2T-mKnMA"
      },
      "source": [
        "#Now make a model with few neurons network 20, 16 and make sure 1 in the end to avoid confusion. So that it shows same prediction each time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVWCdMGS9LlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "00f66ec0-30e7-4ed4-a49c-cd43910ebf33"
      },
      "source": [
        "model = tf.keras.Sequential() #starts with sequestial neurons \n",
        "model.add(hub_layer) #adding pretrained layer. Must check how it was trained by google, just copy paste URL. I already did. \n",
        "model.add(tf.keras.layers.Dense(16,activation=\"relu\")) # relu method \n",
        "model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))# then can switch the method. I will try too. if it imporves the prediction lets see. You also try\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert_embedding (KerasLayer)  (None, 20)                400020    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 400,373\n",
            "Trainable params: 400,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbcSR54YM7_z"
      },
      "source": [
        "#our all neurons are trainable. (20, 16, 1) are neurons and param calculation = input values+ neurons + bias neurons ['read articles about it']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ko813jH9LyH"
      },
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Li2KcgPN7yV"
      },
      "source": [
        "#Compile defines the loss function, the optimizer and the metrics. \n",
        "#That's all. It has nothing to do with the weights and you can compile a model as many times as you want \n",
        "#You need a compiled model to train (because training uses the loss function and the optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3za3mEl8o6R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "7b272ef0-13d3-4fb6-da0f-ddaf904a707f"
      },
      "source": [
        " #model fitting by shuffles 20000 data in batches of 512 in 20 epochs. \n",
        " #epochs - number of times all training vectors are used as per weight \n",
        " #batches are just training samples \n",
        " model.fit(train_data.shuffle(10000).batch(512),epochs=20, validation_data=validation_data.batch(512),verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e6eb0d6e7649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#epochs - number of times all training vectors are used as per weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#batches are just training samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfbY_RxO-3zB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8edc23bf-1289-4221-ab6d-a071c818b689"
      },
      "source": [
        "model.predict([['this is worst movie. Acting was bad']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.393671]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZVhdJc3ABEv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aac01293-193c-4779-f174-24f2f6ee1852"
      },
      "source": [
        "model.predict([['I loved movie, favourite']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7961829]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJV1KGraUw5X"
      },
      "source": [
        "#model is working fine now. #happylearning #SingSongs #DontRushSlowTouch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2uMWjsRUw2Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpo-uTlJShGL"
      },
      "source": [
        "#open for team, welcome, I will try to change the optimiser mannually and hub layer. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El6gM6QAY9i1"
      },
      "source": [
        "Available optimizers\n",
        "1. SGD\n",
        "2. RMSprop\n",
        "3. Adam\n",
        "4. Adadelta\n",
        "5. Adagrad\n",
        "6. Adamax\n",
        "7. Nadam\n",
        "8. Ftrl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb07_Lr9Uuo0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "a2f95b78-8c7f-43a2-f660-08ac12ebbc1b"
      },
      "source": [
        "optimizer_M = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for x, y in train_data:\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass.\n",
        "        logits = model(model = tf.keras.Sequential() #starts with sequestial neurons \n",
        "                       model.add(hub_layer) #adding pretrained layer. Must check how it was trained by google, just copy paste URL. I already did. \n",
        "                       model.add(tf.keras.layers.Dense(16,activation=\"relu\")) # relu method \n",
        "                       model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))# then can switch the method. I will try too. if it imporves the prediction lets see. You also trymodel.summary())\n",
        "        # Loss value for this batch.\n",
        "        loss_value = loss_fn(y, logits)\n",
        "\n",
        "    # Get gradients of loss wrt the weights.\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "    # Update the weights of the model.\n",
        "    optimizer_M.apply_gradients(zip(gradients, model.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-8eff60a2620e>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    model.add(hub_layer) #adding pretrained layer. Must check how it was trained by google, just copy paste URL. I already did.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clLoneaU8pXG"
      },
      "source": [
        "predictivemodel = Sequential()\n",
        "predictivemodel.add(Dense(514, input_dim=514, W_regularizer=WeightRegularizer(l1=0.000001,l2=0.000001), init='normal'))\n",
        "predictivemodel.add(Dense(257, W_regularizer=WeightRegularizer(l1=0.000001,l2=0.000001), init='normal'))\n",
        "predictivemodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}